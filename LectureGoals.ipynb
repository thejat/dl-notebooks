{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L01 : Motivating Applications, Machine Learning Pipeline  (Data, Models, Loss, Optimization), Backpropagation\n",
    "\n",
    " - [Online Demo](https://teachablemachine.withgoogle.com/)\n",
    " - [Micrograd code for backprop](https://github.com/karpathy/micrograd/tree/master), [engine.py](https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py)\n",
    " - [Colab file IO examples](https://colab.research.google.com/notebooks/io.ipynb)\n",
    "   \n",
    "### Goals\n",
    "\n",
    "- Understand the key components to set up a classification task\n",
    "- Relate business problems to machine learning methods\n",
    "- Understand how chain rule works\n",
    "- Understand why multiclass logistic regression may not work well even for 2D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L02 : Feedforward Networks: Nonlinearities, Convolutional Neural Networks: Convolution, Pooling\n",
    "\n",
    " - [Notebook: Pytorch Basics](https://github.com/sotte/pytorch_tutorial)\n",
    " - [Nonlinearities visualization](https://playground.tensorflow.org)\n",
    " - [CNN forward pass visualization](https://adamharley.com/nn_vis/cnn/2d.html)\n",
    " - [NN architecture visualization](https://github.com/lutzroeder/netron)\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "- Get acquainted with the basics of Python\n",
    "- Understand the notion of hidden layers and nonlinearities\n",
    "- Convolution layer as collection of filters applied to input tensors\n",
    "- Why pooling helps in reducing parameters downstream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L03 : Jumpstarting Convolutional Neural Networks: Visualization, Transfer, Practical Models (VGG, ResNet)\n",
    "\n",
    " - [Overfitting and Dropout example](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit)\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "- Understand how to transfer parameters previously learned for a new task\n",
    "- Know the different ways to debug a deep network\n",
    "- Be aware of the different engineering tricks such as dropout, batch normalization\n",
    "- Learn why image datasets can be enhanced using data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L04 : Text and Embeddings: Introduction to NLP, Word Embeddings, Word2Vec\n",
    "\n",
    " - [Spacy](https://spacy.io/usage/spacy-101)\n",
    " - Additional Reading: [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
    " - Additional Reading: CNN for sentence classification tasks. [link1](https://arxiv.org/pdf/1408.5882.pdf) and [link2](https://arxiv.org/pdf/1510.03820v4.pdf)\n",
    " - [Pytorch tutorial on using CNN for sentence classification: notebook 4](https://github.com/bentrevett/pytorch-sentiment-analysis)\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "- Understand how natural language elements (such as words) are processed in an analytics workflow\n",
    "- Understand the shortcomings of methods such as Naive Bayes, Latent Dirichlet Allocation\n",
    "- Realize that a CNN can also be used for a NLP task (sentence classification/sentiment analysis)\n",
    "- What is word2vec and how does it help in NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L05 : Recurrent Neural Networks and Transformers: Sequence to Sequence Learning, RNNs and LSTMs\n",
    "\n",
    " - [RNN example in Pytorch](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)\n",
    " - [RNN function implementation in Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
    " - [LSTM example in Pytorch](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "- Know when prediction tasks can have sequential dependencies\n",
    "- The RNN architecture and unfolding\n",
    "- Know how LSTMs work\n",
    "- Applications of ‘sequential to sequential’ models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L06 : Advanced NLP: Attention, BERT and Transformers\n",
    "\n",
    " - [Attention Is All You Need](https://arxiv.org/abs/1706.03762) and an annotated [version](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    " - [BERT](https://arxiv.org/abs/1810.04805) and its [repository](https://github.com/google-research/bert\n",
    ")\n",
    " - [Illustrated BERT](https://jalammar.github.io/illustrated-bert/)\n",
    " - [Attention](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
    " - [Transformer Tutorial in Pytorch](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    " - [SOTA Transformer Implementations including BERT and DistillBERT](https://github.com/huggingface/transformers) (for example [BERT-base-uncased](https://huggingface.co/bert-base-uncased))\n",
    " - [GPT3 Repository](https://github.com/openai/gpt-3)\n",
    " - [Transformer Visualization](https://poloclub.github.io/transformer-explainer/)\n",
    " - [Paper (2024): Decline of AI Data Commons](https://arxiv.org/abs/2407.14933)\n",
    " - Papers: [Llama](https://arxiv.org/abs/2302.13971), [Llama 2](https://arxiv.org/abs/2307.09288), [Llama 3](https://arxiv.org/abs/2407.21783)\n",
    " - [GPT2 Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "### Goals\n",
    "\n",
    "- Be able to explain self-attention and how it differs from simpler attention mechanisms seen in sequence to sequence models\n",
    "- Be able to reason about keys, values and queries in self-attention\n",
    "- Be able to recall the key characteristics of BERT and how pre-trained models can be used for NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L07 : Unsupervised Deep Learning: Variational Autoencoders\n",
    "\n",
    " - [VAE in Pytorch](https://github.com/pytorch/examples/tree/master/vae) from [Pytorch examples repository](https://github.com/pytorch/examples)\n",
    "\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "- Meaning of generative modeling\n",
    "- What are variational autoencoders (VAEs) and where can they be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L08 : Unsupervised Deep Learning: Generative Adversarial Networks\n",
    "\n",
    " - [Notebook: GAN example on CelebFaces Attributes (CelebA) Dataset](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) ([dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html))\n",
    " - [VAE in Pytorch](https://github.com/pytorch/examples/tree/master/vae) from [Pytorch examples repository](https://github.com/pytorch/examples)\n",
    "- [GAN Demo by Google 2020](https://ai.googleblog.com/2020/11/using-gans-to-create-fantastical.html)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "- The intuition behind generative adversarial networks (GANs)\n",
    "- Differences between GANs and VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L09 : Online Learning: A/B Testing, Multi-armed Bandits, Contextual Bandits \n",
    "\n",
    " - [Bandit Implementations in Python: SMPyBandits](https://smpybandits.github.io/)\n",
    " - [A blog post on Bandits by Lilian Weng](https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html)\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "- What is online learning? How is it different from supervised learning?\n",
    "- Relation between forecasting and decision making\n",
    "- The multi armed bandit problem and solutions\n",
    "- Contextual bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L10 : Reinforcement Learning: Policies, State-Action Value Functions\n",
    "\n",
    " - [Openai Gym](https://github.com/openai/gym)\n",
    " - [RL in Pytorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) from [Pytorch examples repository](https://github.com/pytorch/examples)\n",
    " - [RL with Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)\n",
    " - [RLHF for ChatGPT like Assistants](https://arxiv.org/pdf/2204.05862.pdf)\n",
    "\n",
    "### Goals\n",
    "\n",
    "- What is reinforcement learning?\n",
    "- Basics of Markov Decision Processes\n",
    "- Policies, Value functions and how to think about these two objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L11 : Reinforcement Learning II: Bellman Equations, Q Learning\n",
    "\n",
    " - [Implementations of RL](https://github.com/dennybritz/reinforcement-learning)\n",
    " - [Flappy Bird with Q learning](http://sarvagyavaish.github.io/FlappyBirdRL/)\n",
    " - [ML Katas: Cliffworld with Q learning](https://www.bpesquet.fr/mlkatas/coding/q_learning_cliffworld.html)\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "- Be able to understand the difference between Bellman Expectation Equation and Bellman Optimality Equation\n",
    "- Intuitive reasoning for the Q-Learning update rule\n",
    "- Be able to identify relationships between state value functions, state-action value functions and policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L12 : Deep Reinforcement Learning: Function Approximation, DQN for Atari Games,  DQN for Atari Games, MCTS for AlphaGo \n",
    "\n",
    " - [OpenAI Baselines for RL](https://github.com/openai/baselines)\n",
    " - [Cartpole environment with DQN](https://github.com/seungeunrho/minimalRL/blob/master/dqn.py)\n",
    "\n",
    "\n",
    "### Goals\n",
    "\n",
    "- Know the role of function approximation in Q-learning\n",
    "- Be able to understand the key innovations in the DQN model \n",
    "- Identify the differences between Monte Carlo tree search vs Monte Carlo rollouts\n",
    "- Be able to identify key compoments of the AlphaGo (and variants such as AlphaZero) Go playing agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
