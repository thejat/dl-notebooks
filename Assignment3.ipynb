{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDS 576: Assignment 3 (10 points)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this assignment, you will:\n",
    "\n",
    "- **Understand language modeling fundamentals**: Compare classical n-gram approaches with neural language models and analyze their trade-offs in terms of computational cost, memory requirements, and generation quality.\n",
    "\n",
    "- **Implement and train RNN-based models**: Gain hands-on experience building LSTM-based language models, understanding how to adapt sequence-to-sequence architectures for different NLP tasks.\n",
    "\n",
    "- **Build sequence-to-sequence translation systems**: Implement encoder-decoder architectures for machine translation, incorporating pre-trained word embeddings (GloVe, FastText) to improve model performance.\n",
    "\n",
    "- **Evaluate NLP models using standard metrics**: Apply perplexity for language model evaluation and BLEU score for translation quality assessment, understanding what these metrics capture about model performance.\n",
    "\n",
    "- **Visualize and interpret attention mechanisms**: Analyze how attention weights distribute across input sequences and interpret what the model learns to focus on during translation.\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Guidelines\n",
    "\n",
    "- Turn in solutions as a single notebook (ipynb) and as a pdf on the submission site. No need to turn in datasets/word-docs.\n",
    "\n",
    "- **Notebook structure**: Organize your notebook with clear section headers matching the question numbers. Include all code cells, output cells, and markdown explanations.\n",
    "\n",
    "- **Naming convention**: Name your submission file as `Assignment3_<YourNetID>.ipynb` and `Assignment3_<YourNetID>.pdf`.\n",
    "\n",
    "- Answer the following questions concisely, in complete sentences and with full clarity. If in doubt, ask classmates and the teaching staff. Across group collaboration is not allowed. Always cite all your sources.\n",
    "\n",
    "- **Code requirements**: Include all necessary imports at the top of your notebook. Ensure your code runs end-to-end without errors. Set random seeds where applicable for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 1: RNN for Language Modeling (4 pt)\n",
    "\n",
    "Build and compare language models using the IMDB dataset.\n",
    "\n",
    "**Sub-questions:**\n",
    "\n",
    "- **(a) Data Preparation and N-gram Model (1 pt)**: Import the torchtext IMDB dataset and build a Markov (n-gram) language model.\n",
    "  - Implement at least a bigram and trigram model\n",
    "  - Handle unknown words appropriately using smoothing techniques\n",
    "  - Document your vocabulary size and any preprocessing steps (lowercasing, tokenization, etc.)\n",
    "\n",
    "- **(b) LSTM Language Model (1.5 pt)**: Adapt the architecture from [Seq2Seq_LSTM_Simple_Sentiment_Analysis.ipynb](examples/M05_recurrent/Seq2Seq_LSTM_Simple_Sentiment_Analysis.ipynb) to build an LSTM-based language model.\n",
    "  - Modify the model output layer for next-word prediction\n",
    "  - Train for at least 5 epochs (or until convergence)\n",
    "  - Plot training loss as a function of epochs/iterations\n",
    "\n",
    "- **(c) Design Choices Discussion (0.75 pt)**: For each model, describe the key design choices made. Briefly mention how each choice influences training time and generative quality.\n",
    "  - Discuss: n-gram order, embedding dimension, hidden size, number of layers, dropout rate, learning rate\n",
    "\n",
    "- **(d) Text Generation (0.75 pt)**: For each model, starting with the phrase \"My favorite movie \", sample the next few words and create an approximately 20-word generated review. Repeat this 5 times (you should ideally get different outputs each time) and report the outputs.\n",
    "  - Use temperature-based sampling for the LSTM model\n",
    "  - Compare the coherence and diversity of outputs between models\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Code cell: Data loading and preprocessing pipeline\n",
    "- [ ] Code cell: N-gram language model implementation with smoothing\n",
    "- [ ] Code cell: LSTM language model architecture definition\n",
    "- [ ] Figure: Training loss curve for the LSTM model (properly labeled axes)\n",
    "- [ ] Table: Summary of design choices for both models\n",
    "- [ ] Output: 5 generated reviews from each model (10 total), clearly labeled\n",
    "- [ ] Markdown cell: Discussion of design choices and their impact\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Full credit: Both models implemented correctly, training curve shows convergence, generated text is sensible, and design choices are thoroughly discussed\n",
    "- Partial credit: Models work but with minor issues, incomplete discussion, or limited generation quality\n",
    "- Minimal credit: Only one model implemented, or significant errors in implementation\n",
    "\n",
    "Note: Make any reasonable assumptions as necessary and document them clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 2: Sequence to Sequence Model for Translation (4 pt)\n",
    "\n",
    "Build translation models between English and a language of your choice.\n",
    "\n",
    "**Sub-questions:**\n",
    "\n",
    "- **(a) Model 1: X → English (1.5 pt)**: Train a sequence-to-sequence model building on the [example notebook](examples/M05_recurrent/Seq2Seq_Translation_Example.ipynb) (also available [here](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)) for a language pair (excluding French-English), where the output is English and the input is a language of your choice from [this collection](https://www.manythings.org/anki/).\n",
    "  - Document which language pair you selected and why\n",
    "  - Report the dataset size (number of sentence pairs)\n",
    "  - Train until validation loss plateaus or for at least 20,000 iterations\n",
    "\n",
    "- **(b) Model 2: English → X with Pre-trained Embeddings (1.5 pt)**: Train another model for the reverse direction (English to your chosen language). Use GloVe 100-dimensional embeddings for the English encoder. Use [FastText](https://fasttext.cc/docs/en/crawl-vectors.html) embeddings for the target language if available.\n",
    "  - Compare training dynamics with and without pre-trained embeddings (at least qualitatively)\n",
    "  - Document embedding coverage (what percentage of vocabulary has pre-trained vectors)\n",
    "\n",
    "- **(c) Round-Trip Translation (1 pt)**: Input 5 well-formed sentences in English to Model 2, then input the resulting translated sentences to Model 1. Display all model outputs in each case.\n",
    "  - Select sentences of varying complexity (simple, compound, with idioms, etc.)\n",
    "  - Analyze where the round-trip translation succeeds and fails\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Code cell: Data loading for chosen language pair\n",
    "- [ ] Code cell: Model 1 (X → English) architecture and training\n",
    "- [ ] Code cell: Model 2 (English → X) architecture with pre-trained embeddings\n",
    "- [ ] Figure: Training loss curves for both models\n",
    "- [ ] Table: 5 English sentences, their translations (Model 2 output), and back-translations (Model 1 output)\n",
    "- [ ] Markdown cell: Analysis of translation quality and failure modes\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Full credit: Both models trained successfully, pre-trained embeddings properly integrated, round-trip analysis is insightful\n",
    "- Partial credit: Models work but embeddings not properly loaded, or superficial analysis\n",
    "- Minimal credit: Only one direction implemented, or significant training issues\n",
    "\n",
    "Note: Make any reasonable assumptions as necessary and document them clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 3: Model Evaluation with Perplexity and BLEU Score (1.5 pt)\n",
    "\n",
    "Apply standard NLP evaluation metrics to your models from Questions 1 and 2.\n",
    "\n",
    "**Sub-questions:**\n",
    "\n",
    "- **(a) Perplexity for Language Models (0.75 pt)**: Compute and compare the perplexity of your n-gram and LSTM language models on a held-out test set from the IMDB dataset.\n",
    "  - Use at least 1000 sentences for evaluation\n",
    "  - Report perplexity for bigram, trigram, and LSTM models\n",
    "  - Explain what lower/higher perplexity indicates about model quality\n",
    "\n",
    "- **(b) BLEU Score for Translation (0.75 pt)**: Compute BLEU scores for your translation models using a held-out test set.\n",
    "  - Report BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores\n",
    "  - Compare scores between Model 1 and Model 2\n",
    "  - Discuss what the BLEU scores reveal about translation quality\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Code cell: Perplexity computation for all language models\n",
    "- [ ] Code cell: BLEU score computation for translation models\n",
    "- [ ] Table: Perplexity comparison (bigram, trigram, LSTM)\n",
    "- [ ] Table: BLEU scores (BLEU-1 through BLEU-4) for both translation directions\n",
    "- [ ] Markdown cell: Interpretation of metrics and what they reveal about model quality\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Full credit: Correct implementation of both metrics, proper test set usage, insightful interpretation\n",
    "- Partial credit: Metrics computed but with minor errors, or shallow interpretation\n",
    "- Minimal credit: Only one metric computed, or incorrect implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 4: Attention Visualization (0.5 pt)\n",
    "\n",
    "Visualize and interpret attention patterns in your sequence-to-sequence translation model.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Modify Model 1 or Model 2 from Question 2 to include an attention mechanism (if not already present in your implementation)\n",
    "\n",
    "- For 3 example sentences of varying lengths:\n",
    "  - Generate a heatmap showing attention weights between source and target tokens\n",
    "  - Rows should represent target (output) tokens, columns should represent source (input) tokens\n",
    "  - Use a clear colormap (e.g., viridis or hot) with a colorbar\n",
    "\n",
    "- Analyze the attention patterns:\n",
    "  - Do attention weights align with expected word correspondences?\n",
    "  - How does the model handle word reordering between languages?\n",
    "  - Are there any surprising or incorrect attention patterns?\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Code cell: Attention mechanism implementation (if modifying base model)\n",
    "- [ ] Code cell: Attention weight extraction and visualization function\n",
    "- [ ] Figure: 3 attention heatmaps with proper labels (source tokens on x-axis, target tokens on y-axis)\n",
    "- [ ] Markdown cell: Analysis of attention patterns and what they reveal about the translation process\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Full credit: Clear, properly labeled heatmaps with insightful analysis of attention patterns\n",
    "- Partial credit: Heatmaps generated but with labeling issues or shallow analysis\n",
    "- Minimal credit: Incorrect visualization or no analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hints\n",
    "\n",
    "### Question 1 Hints\n",
    "\n",
    "- **Data loading**: Use `torchtext.datasets.IMDB` or the HuggingFace datasets library (`datasets.load_dataset('imdb')`)\n",
    "- **N-gram smoothing**: Implement Laplace (add-1) smoothing or Kneser-Ney smoothing to handle unseen n-grams\n",
    "- **LSTM for language modeling**: Change the output dimension to vocabulary size and use `CrossEntropyLoss`. The target at each timestep is the next word.\n",
    "- **Temperature sampling**: Divide logits by temperature before softmax; lower temperature (0.5-0.8) = more conservative, higher (1.2-1.5) = more diverse\n",
    "- **Perplexity formula**: $PPL = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\log P(w_i|w_1,...,w_{i-1})\\right)$\n",
    "\n",
    "```python\n",
    "# Useful imports for Question 1\n",
    "from collections import Counter, defaultdict\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "```\n",
    "\n",
    "### Question 2 Hints\n",
    "\n",
    "- **Language pair selection**: Spanish, German, and Italian have good data availability. Avoid French (excluded) and very low-resource languages.\n",
    "- **Loading GloVe embeddings**: Use `torchtext.vocab.GloVe` or download from [Stanford NLP](https://nlp.stanford.edu/projects/glove/)\n",
    "- **Embedding initialization**: For words not in GloVe/FastText, initialize with random vectors or the mean of all embeddings\n",
    "- **Freezing vs. fine-tuning embeddings**: Try both `embedding.weight.requires_grad = False` (frozen) and `True` (fine-tuned)\n",
    "\n",
    "```python\n",
    "# Loading GloVe embeddings\n",
    "from torchtext.vocab import GloVe\n",
    "glove = GloVe(name='6B', dim=100)\n",
    "\n",
    "# Loading FastText\n",
    "import fasttext.util\n",
    "fasttext.util.download_model('de', if_exists='ignore')  # German example\n",
    "ft = fasttext.load_model('cc.de.300.bin')\n",
    "```\n",
    "\n",
    "### Question 3 Hints\n",
    "\n",
    "- **Perplexity computation**: Use `torch.exp(loss)` where loss is the average cross-entropy loss\n",
    "- **BLEU score**: Use `nltk.translate.bleu_score` or `sacrebleu` library for standardized computation\n",
    "- **Test set size**: Use at least 500-1000 examples for reliable metric computation\n",
    "\n",
    "```python\n",
    "# BLEU score computation\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "\n",
    "# For single sentence\n",
    "reference = [['this', 'is', 'a', 'test']]\n",
    "candidate = ['this', 'is', 'test']\n",
    "score = sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
    "```\n",
    "\n",
    "### Question 4 Hints\n",
    "\n",
    "- **Attention mechanism**: The example notebook already includes attention. Extract weights from `decoder_attention` or similar.\n",
    "- **Visualization**: Use `matplotlib.pyplot.imshow()` with proper extent and labels\n",
    "- **Expected patterns**: Diagonal patterns indicate word-by-word correspondence; off-diagonal indicates reordering\n",
    "\n",
    "```python\n",
    "# Attention visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## FAQ\n",
    "\n",
    "**Q: Do I need a GPU for this assignment?**\n",
    "A: A GPU is recommended but not required. On CPU, expect LSTM training to take 1-2 hours for Question 1 and 2-4 hours for Question 2. On a GPU (e.g., Google Colab), training should complete in 15-30 minutes per model. You can reduce training iterations if time-constrained, but document this choice.\n",
    "\n",
    "**Q: What if my GloVe/FastText embedding coverage is low?**\n",
    "A: It's acceptable if 70-90% of your vocabulary is covered by pre-trained embeddings. For out-of-vocabulary words, initialize randomly using `torch.randn()` scaled appropriately, or use the mean of all embeddings. Document your coverage percentage and OOV handling strategy.\n",
    "\n",
    "**Q: How long should my generated reviews be for Question 1?**\n",
    "A: Aim for approximately 20 words as stated, but anywhere between 15-30 words is acceptable. The key is that the generation should demonstrate the model's capability to produce coherent text.\n",
    "\n",
    "**Q: Which language should I choose for translation?**\n",
    "A: Choose any language except French that is available in the [Anki dataset](https://www.manythings.org/anki/). Popular choices include Spanish, German, Italian, Portuguese, or Dutch. Consider your familiarity with the language for manual evaluation of translations.\n",
    "\n",
    "**Q: What if my BLEU scores are very low?**\n",
    "A: BLEU scores for neural machine translation on small datasets can be quite low (5-15 is common for these models). Focus on explaining why the scores are what they are, and what could be done to improve them. Low scores are acceptable as long as your analysis is thoughtful.\n",
    "\n",
    "**Q: Can I use attention in my seq2seq model from the start?**\n",
    "A: Yes, the example notebook already includes attention. You're encouraged to use attention for better performance. Question 4 specifically asks you to visualize these attention weights.\n",
    "\n",
    "**Q: How should I handle sentences of different lengths?**\n",
    "A: Use padding and masking. Pack sequences using `torch.nn.utils.rnn.pack_padded_sequence()` for efficiency, or use simple padding with appropriate mask handling in your loss computation.\n",
    "\n",
    "**Q: What assumptions can I make about preprocessing?**\n",
    "A: You can assume:\n",
    "- Lowercasing all text is acceptable\n",
    "- Basic tokenization (splitting on whitespace and punctuation) is sufficient\n",
    "- Limiting vocabulary to top 10,000-50,000 words is acceptable\n",
    "- Using sentence length limits (e.g., max 50 tokens) for training efficiency is acceptable\n",
    "\n",
    "**Q: My perplexity values seem too high or too low. What's expected?**\n",
    "A: For a well-trained LSTM language model on IMDB, expect perplexity in the range of 50-200. N-gram models typically have higher perplexity (100-500). If you get perplexity below 10, check for data leakage; if above 1000, check your loss computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "### Course Materials\n",
    "\n",
    "- [Sequence-to-Sequence and RNNs Slides](slides/M05a_recurrent.pdf) - Core concepts for this assignment\n",
    "- [Attention Mechanisms Slides](slides/M05b_attention.pdf) - Understanding attention for Question 4\n",
    "- [NLP Fundamentals Slides](slides/M04_text.pdf) - Word embeddings and language modeling basics\n",
    "- [Advanced Text Processing Slides](slides/M06a_transformers.pdf) - Additional context on NLP techniques\n",
    "\n",
    "### Example Notebooks\n",
    "\n",
    "- [Seq2Seq LSTM Sentiment Analysis](examples/M05_recurrent/Seq2Seq_LSTM_Simple_Sentiment_Analysis.ipynb) - Base for Question 1 LSTM model\n",
    "- [Seq2Seq Translation Example](examples/M05_recurrent/Seq2Seq_Translation_Example.ipynb) - Base for Question 2\n",
    "- [Seq2Seq RNN Sentiment Analysis](examples/M05_recurrent/Seq2Seq_RNN_Simple_Sentiment_Analysis.ipynb) - Alternative RNN architecture reference\n",
    "\n",
    "### External Resources\n",
    "\n",
    "- [PyTorch Seq2Seq Translation Tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) - Official PyTorch tutorial\n",
    "- [Anki Parallel Corpora](https://www.manythings.org/anki/) - Translation dataset source\n",
    "- [GloVe Embeddings](https://nlp.stanford.edu/projects/glove/) - Pre-trained English word vectors\n",
    "- [FastText Embeddings](https://fasttext.cc/docs/en/crawl-vectors.html) - Pre-trained multilingual word vectors\n",
    "- [NLTK BLEU Score Documentation](https://www.nltk.org/api/nltk.translate.bleu_score.html) - BLEU implementation reference\n",
    "- [SacreBLEU Library](https://github.com/mjpost/sacrebleu) - Standardized BLEU computation\n",
    "\n",
    "### Dataset Download Instructions\n",
    "\n",
    "**IMDB Dataset (Question 1):**\n",
    "```python\n",
    "# Option 1: Using torchtext\n",
    "from torchtext.datasets import IMDB\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "\n",
    "# Option 2: Using HuggingFace datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('imdb')\n",
    "```\n",
    "\n",
    "**Translation Dataset (Question 2):**\n",
    "```python\n",
    "# Download from https://www.manythings.org/anki/\n",
    "# Example for German-English:\n",
    "!wget https://www.manythings.org/anki/deu-eng.zip\n",
    "!unzip deu-eng.zip\n",
    "\n",
    "# Or use the data loading from the example notebook\n",
    "```\n",
    "\n",
    "**GloVe Embeddings:**\n",
    "```python\n",
    "# Option 1: Using torchtext\n",
    "from torchtext.vocab import GloVe\n",
    "glove = GloVe(name='6B', dim=100)  # Downloads ~800MB\n",
    "\n",
    "# Option 2: Manual download\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "# Recommended: Start with necessary imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
