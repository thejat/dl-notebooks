{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IDS 576: Assignment 4\n",
    "\n",
    "- Turn in solutions as a single notebook (ipynb) and as a pdf on Blackboard. No need to turn in datasets/word-docs.\n",
    "- Answer the following questions concisely, in complete sentences and with full clarity. If in doubt, ask classmates and the teaching staff. Across group collaboration is not allowed. Always cite all your sources."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1. Bandits and RL (2pt)\n",
    "The following questions do not involve programming. You can use the markdown option for cells in Jupyter notebook to answer.\n",
    "\n",
    " -  What is the difference between A/B testing and Multi-armed bandits?\n",
    " -  What is the role of exploration in the Bandit problems?\n",
    " -  What is the difference between the UCB and the Thompson sampling methods in terms of exploration?\n",
    " -  How does the contextual setting differ from the non-contextual setting in terms of difficulty (be precise)? \n",
    " -  Can bandit algorithms be used for contextual bandits setting? If so, what is the disadvantage?\n",
    " -  What is the difference between a Markov Reward Process and a Markov Decision Process? Can Bellman Expectation Equation be applied to both?\n",
    " -  What is the difference between supervised learning and reinforcement learning?\n",
    " -  How are simulations used in a forward search? (i.e., in a simple Monte Carlo search)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2. Bandits (3pt)\n",
    "\n",
    "Consider a $5$-armed stochastic bandit problem with mean rewards of $(0.1,0.1,0.1,0.1,0.9)$. The arms are Bernoulli. \n",
    " -  Write a function that responds with a stochastically generated reward given the arm index as an input. We will use it to test the performance of various algorithms next.\n",
    " -  Write individual functions for epsilon-greedy, UCB1 (informally also referred to as UCB) and Thompson sampling (use Beta-Bernoulli conjugacy) from scratch.\n",
    " -  For various choices of $\\epsilon$, show how epsilon-greedy performs in terms of cumulative expected regret and in terms of arm selection.\n",
    " -  Plot multiple simulations of the performance of UCB1 algorithm.\n",
    " -  Plot multiple simulations of the performance of Thompson sampling algorithm. Comment on which algorithm is better qualitatively."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3. Reinforcement Learning (3pt)\n",
    "\n",
    "We will use the MIT licensed [code](https://github.com/seungeunrho/minimalRL) to do sensitivity analysis of DQN for the cartpole environment from the `gym` [package](https://gym.openai.com/). You should clone it as needed.\n",
    "\n",
    " -  Describe the state, actions, transitions and rewards for the cartpole environment using the gym package documentation.\n",
    " -  Describe the Q-network used in `dqn.py`. What are the layers and what are the outputs?\n",
    " -  Run the default DQN configuration for cartpole in `dqn.py` and plot the (25,50,75)-percentile reward performance curves over multiple simulations/runs.\n",
    " -  Change the epsilon value (for exploration) to fixed values $\\{0.01,0.1\\}$ and plot its impact on learning. Provide an interpretation of the trend observed.\n",
    " -  Change the buffer\\_limit (of the experience replay buffer) to $\\{5000,10000,25000\\}$ and plot its impact on learning. Provide an interpretation of the trend observed.\n",
    " -  Change gamma (for discounting) to $\\{0.75,0.9\\}$ and discuss its impact on on learning. Provide an interpretation of the trend observed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}