{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example LSTM model for sentiment prediction on the IMDB dataset\n",
    "Source: [blogpost](https://github.com/hassaanbinaslam/myblog/blob/main/posts/2022-11-09-pytorch-lstm-imdb-sentiment-prediction.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - pytorch\n",
      " - nvidia\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 pytorch-cuda=12.1 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: pandas in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: portalocker in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (3.1.1)\n",
      "Requirement already satisfied: torchtext==0.18 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: datasets in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: tqdm in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from torchtext==0.18) (4.67.1)\n",
      "Requirement already satisfied: requests in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from torchtext==0.18) (2.32.3)\n",
      "Requirement already satisfied: torch>=2.3.0 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from torchtext==0.18) (2.3.0)\n",
      "Requirement already satisfied: numpy in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from torchtext==0.18) (2.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: filelock in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from requests->torchtext==0.18) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from requests->torchtext==0.18) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from requests->torchtext==0.18) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from requests->torchtext==0.18) (2025.1.31)\n",
      "Requirement already satisfied: sympy in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from torch>=2.3.0->torchtext==0.18) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from torch>=2.3.0->torchtext==0.18) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from torch>=2.3.0->torchtext==0.18) (3.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from jinja2->torch>=2.3.0->torchtext==0.18) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/theja/miniconda3/envs/theja/lib/python3.12/site-packages (from sympy->torch>=2.3.0->torchtext==0.18) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib pandas portalocker torchtext==0.18 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python==3.12.9\n",
      "numpy==2.0.1\n",
      "torch==2.3.0\n",
      "torchtext==0.18.0+cpu\n",
      "matplotlib==3.10.1\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "import numpy, matplotlib, pandas, torch, torchtext\n",
    "\n",
    "print(\"python==\" + python_version())\n",
    "print(\"numpy==\" + numpy.__version__)\n",
    "print(\"torch==\" + torch.__version__)\n",
    "print(\"torchtext==\" + torchtext.__version__)\n",
    "print(\"matplotlib==\" + matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theja/miniconda3/envs/theja/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "train_dataset_raw = load_dataset(\"imdb\", split=\"train\")\n",
    "test_dataset_raw = load_dataset(\"imdb\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hkKuykqX-zB"
   },
   "source": [
    "Check the size of the downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DuEsWZ4QwPop",
    "outputId": "15d68147-95cf-458c-a25c-b9aeec98e462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:  25000\n",
      "Test dataset size:  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset size: \", len(list(train_dataset_raw)))\n",
    "print(\"Test dataset size: \", len(list(test_dataset_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_raw_processed = []\n",
    "for el in train_dataset_raw:\n",
    "    y = \"pos\" if el[\"label\"] == 1 else \"neg\"\n",
    "    train_dataset_raw_processed.append((y, el[\"text\"]))\n",
    "test_dataset_raw_processed = []\n",
    "for el in test_dataset_raw:\n",
    "    y = \"pos\" if el[\"label\"] == 1 else \"neg\"\n",
    "    test_dataset_raw_processed.append((y, el[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs1BlO8aYH2L"
   },
   "source": [
    "### Split train data further into train and validation set\n",
    "\n",
    "Both train and test datasets have 25000 reviews. Therefore, we can split the training set further into the train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K7L2Shr4wJqw"
   },
   "outputs": [],
   "source": [
    "train_set_size = 20000\n",
    "valid_set_size = 5000\n",
    "\n",
    "train_dataset, valid_dataset = random_split(list(train_dataset_raw_processed), [20000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-mX3i1Z1orIr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\W'\n",
      "/tmp/ipykernel_40445/2976894019.py:11: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text)\n",
      "/tmp/ipykernel_40445/2976894019.py:12: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  text = re.sub(\"[\\W]+\", \" \", text)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def tokenizer(text):\n",
    "    # step 1. remove HTML tags. they are not helpful in understanding the sentiments of a review\n",
    "    # step 2: use lowercase for all text to keep symmetry\n",
    "    # step 3: extract emoticons. keep them as they are important sentiment signals\n",
    "    # step 4: remove punctuation marks\n",
    "    # step 5: put back emoticons\n",
    "    # step 6: generate word tokens\n",
    "    text = re.sub(\"<[^>]*>\", \"\", text)\n",
    "    text = text.lower()\n",
    "    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text)\n",
    "    text = re.sub(\"[\\W]+\", \" \", text)\n",
    "    text = text + \" \".join(emoticons).replace(\"-\", \"\")\n",
    "    tokenized = text.split()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nrsi4alivzsn",
    "outputId": "48bd4363-f820-4a43-93dc-0bf56c35eb0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB vocab size: 69023\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# step 1: convert reviews into tokens\n",
    "# step 2: find frequency of tokens\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    " \n",
    "print('IMDB vocab size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuaysCgS-sgY",
    "outputId": "2da48263-8c05-456e-9e30-5e595919a618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this  -->  11\n",
      "is  -->  7\n",
      "an  -->  35\n",
      "example  -->  457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theja/miniconda3/envs/theja/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/theja/miniconda3/envs/theja/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# step 3: sort the token based on their frequency\n",
    "# step 4: put the sorted tokens in OrderedDict\n",
    "# step 5: convert token to integers using vocab object\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "vb = vocab(ordered_dict)\n",
    "\n",
    "vb.insert_token(\"<pad>\", 0)  # special token for padding\n",
    "vb.insert_token(\"<unk>\", 1)  # special token for unknown words\n",
    "vb.set_default_index(1)\n",
    "\n",
    "# print some token indexes from vocab\n",
    "for token in [\"this\", \"is\", \"an\", \"example\"]:\n",
    "    print(token, \" --> \", vb[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SuaVYQ-sgttl"
   },
   "outputs": [],
   "source": [
    "##\n",
    "# inline lambda functions for text and label precessing\n",
    "text_pipeline = lambda x: [vb[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1.0 if x == \"pos\" else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAthQbaE9f4E",
    "outputId": "5d3efdc2-19ae-4615-8db6-4e652d720822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9UViOgiB6JBC"
   },
   "outputs": [],
   "source": [
    "##\n",
    "# a function to apply pre-processing steps at a batch level\n",
    "import torch.nn as nn\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "\n",
    "    # iterate over all reviews in a batch\n",
    "    for _label, _text in batch:\n",
    "        # label preprocessing\n",
    "        # print(f\"label is {_label}\")\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        # text preprocessing\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "\n",
    "        # store the processed text in a list\n",
    "        text_list.append(processed_text)\n",
    "        \n",
    "        # store the length of processed text\n",
    "        # this will come handy in future when we want to know the original size of a text (without padding)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    \n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    \n",
    "    # pad the processed reviews to make their lengths consistant\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
    "        text_list, batch_first=True)\n",
    "    \n",
    "    # return\n",
    "    # 1. a list of processed and padded review texts\n",
    "    # 2. a list of processed labels\n",
    "    # 3. a list of review text original lengths (before padding)\n",
    "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LH9VlvnlTH6l"
   },
   "source": [
    "## Batching the training, validation, and test dataset\n",
    "\n",
    "Let's proceed on creating DataLoaders for train, valid, and test data with `batch_size = 32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CR6eKF3_RbyA"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dl = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    test_dataset_raw_processed, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnuIywwGTsqF"
   },
   "source": [
    "## Define model training and evaluation pipelines\n",
    "I have defined two simple functions to train and evaluate the model in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GpTak2XHTi_L"
   },
   "outputs": [],
   "source": [
    "##\n",
    "# model training pipeline\n",
    "# https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "# model evaluation pipeline\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQrhPBw3TZ2o"
   },
   "source": [
    "## RNN model configuration, loss function, and optimizer\n",
    "We have seen the review text, which can be long sequences. We will use the LSTM layer for capturing the long-term dependencies. Our sentiment analysis model is composed of the following layers\n",
    "\n",
    "* Start with an **Embedding layer**. Placing the embedding layer is similar to one-hot-encoding, where each word token is converted to a separate feature (or vector or column). But this can lead to too many features (curse of dimensionality or dimensional explosion). To avoid this, we try to map tokens to fixed-size vectors (or columns). In such a feature matrix, different elements denote different tokens. Tokens that are closed are also placed together. Further, during training, we also learn and update the positioning of tokens. Similar tokens are placed into closer and closer locations. Such a matrix layer is termed an embedding layer.\n",
    "* After the embedding layer, there is the RNN layer (LSTM to be specific).\n",
    "* Then we have a fully connected layer followed by activation and another fully connected layer.\n",
    "* Finally, we have a logistic sigmoid layer for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "k36IeQGJTT3a"
   },
   "outputs": [],
   "source": [
    "##\n",
    "# https://github.com/rasbt/machine-learning-book/blob/main/ch15/ch15_part2.ipynb\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n",
    "        )\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "K8KLG602WJUh"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vb)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxKYEeNwT1vL"
   },
   "source": [
    "### Define model loss function and optimizer\n",
    "For loss function (or criterion), I have used [Binary Cross Entropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html), and for loss optimization, I have used [Adam algorithm](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QV0gDqZETw3-"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqGMq6JuaFSV"
   },
   "source": [
    "## Model training and evaluation\n",
    "Let's run the pipeline for ten epochs and compare the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5PGkqLpT6MX",
    "outputId": "3c53356b-77ab-49e2-92ff-5e8086d3b9c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train accuracy: 0.6096; val accuracy: 0.6852\n",
      "Epoch 1 train accuracy: 0.7257; val accuracy: 0.7452\n",
      "Epoch 2 train accuracy: 0.7466; val accuracy: 0.6284\n",
      "Epoch 3 train accuracy: 0.7253; val accuracy: 0.5366\n",
      "Epoch 4 train accuracy: 0.7972; val accuracy: 0.7492\n",
      "Epoch 5 train accuracy: 0.8619; val accuracy: 0.7784\n",
      "Epoch 6 train accuracy: 0.8911; val accuracy: 0.8040\n",
      "Epoch 7 train accuracy: 0.9162; val accuracy: 0.8574\n",
      "Epoch 8 train accuracy: 0.9328; val accuracy: 0.8598\n",
      "Epoch 9 train accuracy: 0.9504; val accuracy: 0.8634\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid = evaluate(valid_dl)\n",
    "    print(\n",
    "        f\"Epoch {epoch} train accuracy: {acc_train:.4f}; val accuracy: {acc_valid:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6swkupX8DpRC"
   },
   "source": [
    "### Evaluate sentiments on random texts\n",
    "Let's create another helper method to evaluate sentiments on random texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zk4VwWGV_BhV"
   },
   "outputs": [],
   "source": [
    "def classify_review(text):\n",
    "    text_list, lengths = [], []\n",
    "\n",
    "    # process review text with text_pipeline\n",
    "    # note: \"text_pipeline\" has dependency on data vocabulary\n",
    "    processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "    text_list.append(processed_text)\n",
    "\n",
    "    # get processed review tokens length\n",
    "    lengths.append(processed_text.size(0))\n",
    "    lengths = torch.tensor(lengths)\n",
    "        \n",
    "    # change the dimensions from (torch.Size([8]), torch.Size([1, 8]))\n",
    "    # nn.utils.rnn.pad_sequence(text_list, batch_first=True) does this too\n",
    "    padded_text_list = torch.unsqueeze(processed_text, 0)\n",
    "\n",
    "    # move tensors to correct device\n",
    "    padded_text_list = padded_text_list.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "\n",
    "    # get prediction\n",
    "    model.eval()\n",
    "    pred = model(padded_text_list, lengths)\n",
    "    print(\"model pred: \", pred)\n",
    "\n",
    "    # positive or negative review\n",
    "    review_class = 'negative' # else case\n",
    "    if (pred>=0.5) == 1:\n",
    "        review_class = \"positive\"\n",
    "\n",
    "    print(\"review type: \", review_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "xqX24x-IzErD"
   },
   "outputs": [],
   "source": [
    "##\n",
    "# create two random texts with strong positive and negative sentiments\n",
    "pos_review = 'i love this movie. it was so good.'\n",
    "neg_review = 'slow and boring. waste of time.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oejN6rJ_pS8",
    "outputId": "0f2c5b34-3ce2-4304-da0a-30855050c1dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model pred:  tensor([[0.8522]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "review type:  positive\n"
     ]
    }
   ],
   "source": [
    "classify_review(pos_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_M1VhbMBygl",
    "outputId": "2dee82f4-b8fa-44f8-d5aa-ffddc54488d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model pred:  tensor([[0.0029]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "review type:  negative\n"
     ]
    }
   ],
   "source": [
    "classify_review(neg_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "theja",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
