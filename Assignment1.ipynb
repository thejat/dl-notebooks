{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDS 576: Assignment 1 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By completing this assignment, you will:\n",
    "\n",
    "1. **Understand backpropagation mechanics**: Construct computation graphs and manually compute gradients using the chain rule, building intuition for how automatic differentiation works in deep learning frameworks.\n",
    "\n",
    "2. **Implement gradient descent from scratch**: Develop a working understanding of optimization by implementing gradient descent for linear and nonlinear models, observing convergence behavior under different learning rates.\n",
    "\n",
    "3. **Master the classification pipeline**: Build an end-to-end machine learning workflow including data splitting, model training, cross-validation, hyperparameter tuning, and regularization.\n",
    "\n",
    "4. **Compare neural network architectures and optimizers**: Gain practical experience with feedforward neural networks in Keras/TensorFlow, understanding how activation functions, network depth, and optimizer choice affect model performance.\n",
    "\n",
    "5. **Visualize and interpret model behavior**: Create informative visualizations including decision boundaries, loss curves, and gradient flow diagrams to diagnose and communicate model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Guidelines\n",
    "\n",
    "- **Format**: Submit both a Jupyter notebook (`.ipynb`) and PDF version on the submission site.\n",
    "- **Naming Convention**: `Assignment1_GroupNumber.ipynb`\n",
    "- **Structure**: Organize your notebook with clear section headers matching the question numbers (Q1, Q2, etc.). Each question should have:\n",
    "  - Code cells with comments\n",
    "  - Markdown cells explaining your approach and findings\n",
    "  - Output cells showing results/figures\n",
    "- **Citations**: Always cite all sources (papers, tutorials, Stack Overflow, etc.)\n",
    "- **Collaboration**: Within-group discussion allowed; cross-group collaboration is **not** allowed.\n",
    "- **No need to submit**: Datasets, word documents, or external files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 1: Backpropagation (0.5 pt)\n",
    "\n",
    "Draw the computation graph for the following function:\n",
    "\n",
    "$$f(a,b,c,d,e) = \\frac{1}{(1+(a^b + c^d) \\cdot e)^2}$$\n",
    "\n",
    "Compute the gradient of the function with respect to its inputs at $(a,b,c,d,e) = (1,1,1,1,1)$.\n",
    "\n",
    "**Sub-questions:**\n",
    "- (a) Draw the computation graph showing all intermediate nodes and operations.\n",
    "- (b) Label each edge with the local gradient (partial derivative of output with respect to input for that operation).\n",
    "- (c) Use the chain rule to compute $\\frac{\\partial f}{\\partial a}$, $\\frac{\\partial f}{\\partial b}$, $\\frac{\\partial f}{\\partial c}$, $\\frac{\\partial f}{\\partial d}$, $\\frac{\\partial f}{\\partial e}$ at the given point.\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] A clear hand-drawn or digitally-created computation graph (embedded image or LaTeX-rendered diagram)\n",
    "- [ ] Step-by-step gradient computation showing intermediate values\n",
    "- [ ] Final gradient vector $[\\frac{\\partial f}{\\partial a}, \\frac{\\partial f}{\\partial b}, \\frac{\\partial f}{\\partial c}, \\frac{\\partial f}{\\partial d}, \\frac{\\partial f}{\\partial e}]$\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Correct graph structure with all intermediate nodes (0.2 pt)\n",
    "- Correct local gradients on edges (0.1 pt)\n",
    "- Correct final gradient values via chain rule (0.2 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 2: Gradient Descent (2 pt)\n",
    "\n",
    "**Sub-questions:**\n",
    "\n",
    "- (a) Write a function to compute the mean squared error between a prediction and ground truth assuming both are numpy arrays (see python module `numpy`).\n",
    "\n",
    "- (b) Consider a model: $y = mx + c$, where the model parameter $m = 1$ and parameter $c = 0$ and $x \\in (0,1)$. Plot the function using matplotlib.\n",
    "\n",
    "- (c) Generate example data by drawing $N = 100$ uniform values from the range in which $x$ lies, and compute the corresponding $y$ to get $\\{x_i,y_i\\}_{i=1}^{N}$.\n",
    "\n",
    "- (d) Assuming that you do not know the model parameters, use backpropagation and gradient descent updates to find the model parameters (choose an appropriate learning rate). The loss function will be the mean squared error.\n",
    "\n",
    "- (e) Plot the error in the estimates as a function of the number of iterations of gradient update. Change the learning rate and plot another curve on the previous plot.\n",
    "\n",
    "- (f) Do steps (c)-(e) when the model is $y = m_1x + m_2x^2 + c$ and the true parameters are $m_1 = 0.5$, $m_2 = 1$ and $c = 1$. And $x \\in (0,1)$. Also, plot the ground truth function (i.e., with the true values of $m_1,m_2$ and $c$). Compare and contrast the plot with the previous one.\n",
    "\n",
    "- (g) Do steps (c)-(e) when the model is $y = \\tanh(m \\cdot x + c)$ and the true parameters are $m = 1$ and $c = 2$. And $x \\in (0,2)$. Also, plot the ground truth function.\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] MSE function implementation with docstring\n",
    "- [ ] Plot of linear model $y = mx + c$\n",
    "- [ ] Scatter plot of generated data points\n",
    "- [ ] Code implementing manual gradient descent (no autograd)\n",
    "- [ ] Loss vs. iterations plot with at least **3 different learning rates** (e.g., 0.001, 0.01, 0.1)\n",
    "- [ ] Comparison plot for quadratic model showing ground truth vs. learned function\n",
    "- [ ] Comparison plot for tanh model showing ground truth vs. learned function\n",
    "- [ ] Brief written comparison (2-3 sentences) of convergence behavior across models\n",
    "\n",
    "**Specifications:**\n",
    "- Test at least 3 learning rates per model\n",
    "- Run gradient descent for a minimum of 500 iterations (or until convergence)\n",
    "- All plots must have labeled axes, titles, and legends\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Correct MSE implementation (0.2 pt)\n",
    "- Correct gradient derivation and update rules (0.5 pt)\n",
    "- Learning rate comparison with clear visualization (0.5 pt)\n",
    "- Quadratic model implementation and analysis (0.4 pt)\n",
    "- Tanh model implementation and analysis (0.4 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 3: ML Basics (0.5 pt)\n",
    "\n",
    "**Sub-questions:**\n",
    "\n",
    "- (a) Write a function to compute the (multiclass) logistic loss (also called the cross-entropy loss) given the parameters $(W,b)$ of a linear model (as _numpy_ arrays) and an example $(x,y)$.\n",
    "\n",
    "- (b) Add an $\\ell_1$ regularization and an $\\ell_2$ regularization to the loss function.\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] `cross_entropy_loss(W, b, x, y)` function with docstring\n",
    "- [ ] `cross_entropy_loss_l1(W, b, x, y, lambda_1)` function\n",
    "- [ ] `cross_entropy_loss_l2(W, b, x, y, lambda_2)` function\n",
    "- [ ] Test case demonstrating each function on a simple example\n",
    "\n",
    "**Specifications:**\n",
    "- Handle numerical stability (log of zero) using appropriate techniques (e.g., adding small epsilon or using log-sum-exp trick)\n",
    "- Support multi-class classification (K classes)\n",
    "- Include type hints and docstrings\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Correct softmax + cross-entropy implementation (0.2 pt)\n",
    "- Numerically stable implementation (0.1 pt)\n",
    "- Correct regularization terms (0.2 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 4: Classification Pipeline (2.5 pt)\n",
    "\n",
    "**Sub-questions:**\n",
    "\n",
    "- (a) Generate data from [`Data_Linear_Classifier.ipynb`](examples/M02_feedforward/Data_Linear_Classifier.ipynb) (see the examples folder in the course repo).\n",
    "\n",
    "- (b) Split the data into test and train (20%:80%).\n",
    "\n",
    "- (c) Build a linear classifier assuming the multiclass logistic loss and an $\\ell_2$ regularization for the weights only. Report the prediction accuracy on the training data and the test data and show appropriate plots.\n",
    "\n",
    "- (d) Introduce a cross-validation scheme and justify your choice of parameters. What is the validation accuracy compared to the test accuracy?\n",
    "\n",
    "- (e) What is the sensitivity of the model's performance to different learning rates and the number of gradient descent iterations? Describe via suitable plots.\n",
    "\n",
    "- (f) What is the sensitivity of the model's performance to different regularization parameter values? Find the best regularization parameter using an exhaustive search procedure. Describe your choice via suitable plots. What is the performance difference between using regularization and no regularization?\n",
    "\n",
    "- (g) What is the sensitivity of the model's performance with respect to a different test-train split (e.g., 50%:50%)?\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Data generation and visualization code\n",
    "- [ ] Train/test split implementation (use `sklearn.model_selection.train_test_split`)\n",
    "- [ ] Linear classifier with L2 regularization\n",
    "- [ ] Table: Training accuracy, Test accuracy for baseline model\n",
    "- [ ] K-fold cross-validation implementation (K ≥ 5)\n",
    "- [ ] Learning rate sensitivity plot (test at least 5 values: 1e-4, 1e-3, 1e-2, 1e-1, 1.0)\n",
    "- [ ] Regularization parameter search plot (test at least 5 values spanning 3 orders of magnitude)\n",
    "- [ ] Table comparing performance across different train/test splits\n",
    "- [ ] Summary table of best hyperparameters found\n",
    "\n",
    "**Specifications:**\n",
    "- Use K ≥ 5 for cross-validation\n",
    "- Test at least 5 different learning rates\n",
    "- Test at least 5 different regularization parameters (including 0)\n",
    "- All accuracy values should be reported as percentages with 2 decimal places\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Correct data generation and splitting (0.3 pt)\n",
    "- Working classifier implementation (0.5 pt)\n",
    "- Proper cross-validation setup and analysis (0.5 pt)\n",
    "- Comprehensive hyperparameter sensitivity analysis (0.7 pt)\n",
    "- Clear visualizations and written analysis (0.5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 5: Feedforward Neural Networks (2.5 pt)\n",
    "\n",
    "Consider two models:\n",
    "- **(a)** A 2-layer feedforward neural network (i.e., 1 hidden layer): $f(x,W_1,b_1,W_2,b_2) = W_2\\max(0,W_1x+b_1) + b_2$\n",
    "- **(b)** Same architecture but with Leaky ReLU: $f(x) = x$ if $x > 0$, else $f(x) = 0.01 \\cdot x$\n",
    "\n",
    "**Sub-questions:**\n",
    "\n",
    "- (a) Build the above classifiers using _Keras_ and _TensorFlow_ and solve the classification problem for [MNIST/Fashion MNIST](https://www.tensorflow.org/tutorials/keras/classification).\n",
    "\n",
    "- (b) Discuss how optimizer choice influences performance.\n",
    "\n",
    "- (c) What happens when the number of hidden units chosen is much smaller? Similarly, what happens when the number of hidden units chosen is much higher?\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Keras model definitions for both ReLU and Leaky ReLU architectures\n",
    "- [ ] Training code with training/validation curves\n",
    "- [ ] Table comparing test accuracy: ReLU vs. Leaky ReLU\n",
    "- [ ] Table comparing optimizers: SGD vs. Adam vs. RMSprop (at minimum)\n",
    "- [ ] Table showing performance vs. hidden layer size (test at least 4 sizes: 16, 64, 256, 1024)\n",
    "- [ ] Training loss curves for different hidden layer sizes\n",
    "- [ ] Written analysis (3-5 sentences each) for optimizer and hidden unit experiments\n",
    "\n",
    "**Specifications:**\n",
    "- Use Fashion MNIST dataset (more challenging than MNIST)\n",
    "- Train for at least 10 epochs per configuration\n",
    "- Use batch size of 64 or 128\n",
    "- Report validation accuracy at end of training\n",
    "- Compare at least 3 different optimizers\n",
    "- Test at least 4 different hidden layer sizes\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Correct model implementation (0.5 pt)\n",
    "- Optimizer comparison with analysis (0.7 pt)\n",
    "- Hidden unit size analysis (0.7 pt)\n",
    "- Quality of visualizations and written explanations (0.6 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 6: Decision Boundary Visualization (0.5 pt)\n",
    "\n",
    "Using your trained linear classifier from Question 4, create a visualization of the decision boundaries.\n",
    "\n",
    "**Sub-questions:**\n",
    "\n",
    "- (a) Create a 2D mesh grid covering the input space and predict class labels for each point.\n",
    "\n",
    "- (b) Plot the decision regions using colored contours, overlaying the training data points.\n",
    "\n",
    "- (c) How do the decision boundaries change when you vary the regularization parameter (λ)? Show at least 3 different λ values side-by-side.\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Decision boundary visualization function\n",
    "- [ ] Figure with decision regions and data points for baseline model\n",
    "- [ ] Side-by-side comparison figure showing decision boundaries for 3 different λ values\n",
    "- [ ] Brief written analysis (2-3 sentences) explaining how regularization affects decision boundaries\n",
    "\n",
    "**Specifications:**\n",
    "- Use `plt.contourf()` or similar for decision regions\n",
    "- Include colorbar indicating class predictions\n",
    "- Grid resolution: at least 100x100 points\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Correct decision boundary computation (0.2 pt)\n",
    "- Clear, informative visualization (0.2 pt)\n",
    "- Regularization effect analysis (0.1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 7: Optimizer Comparison Study (1.0 pt)\n",
    "\n",
    "Conduct a systematic comparison of optimization algorithms on your feedforward neural network from Question 5.\n",
    "\n",
    "**Sub-questions:**\n",
    "\n",
    "- (a) Implement training loops with SGD (vanilla), SGD with momentum (0.9), Adam, and RMSprop optimizers.\n",
    "\n",
    "- (b) For each optimizer, plot the training loss and validation accuracy over epochs on the same figure.\n",
    "\n",
    "- (c) Compare convergence speed: How many epochs does each optimizer require to reach 85% validation accuracy?\n",
    "\n",
    "- (d) Compare final performance: What is the best validation accuracy achieved by each optimizer after 20 epochs?\n",
    "\n",
    "- (e) Discuss the trade-offs between the optimizers in terms of convergence speed, final accuracy, and sensitivity to learning rate.\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Training code for all 4 optimizers\n",
    "- [ ] Combined plot: Training loss vs. epoch for all optimizers\n",
    "- [ ] Combined plot: Validation accuracy vs. epoch for all optimizers\n",
    "- [ ] Table: Epochs to reach 85% accuracy (or \"Did not reach\" if applicable)\n",
    "- [ ] Table: Final validation accuracy after 20 epochs\n",
    "- [ ] Learning rate sensitivity analysis for SGD vs. Adam (test 3 learning rates each)\n",
    "- [ ] Written analysis (5-7 sentences) discussing trade-offs\n",
    "\n",
    "**Specifications:**\n",
    "- Use the same random seed for reproducibility across experiments\n",
    "- Use identical network architecture for all optimizer comparisons\n",
    "- Test learning rates: 0.001, 0.01, 0.1 for SGD; 0.0001, 0.001, 0.01 for Adam\n",
    "- Run each configuration for exactly 20 epochs\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Correct implementation of all optimizers (0.3 pt)\n",
    "- Clear convergence comparison plots (0.3 pt)\n",
    "- Learning rate sensitivity analysis (0.2 pt)\n",
    "- Quality of written analysis and insights (0.2 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 8: Gradient Flow Analysis (0.5 pt)\n",
    "\n",
    "Analyze how gradients flow through your neural network during training.\n",
    "\n",
    "**Sub-questions:**\n",
    "\n",
    "- (a) For your 2-layer network from Question 5, record the gradient magnitudes (L2 norm) for each layer during training.\n",
    "\n",
    "- (b) Plot the gradient magnitude for each layer vs. training step for the first 100 batches.\n",
    "\n",
    "- (c) Compare gradient flow between ReLU and Leaky ReLU activations. Which shows more stable gradients?\n",
    "\n",
    "- (d) What happens to gradient magnitudes when you increase the network depth to 5 layers? (Add 3 more hidden layers with the same number of units)\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Code to extract and record gradient magnitudes using `tf.GradientTape` or Keras callbacks\n",
    "- [ ] Plot: Gradient magnitude per layer vs. training step (ReLU network)\n",
    "- [ ] Plot: Gradient magnitude per layer vs. training step (Leaky ReLU network)\n",
    "- [ ] Comparison plot or table: ReLU vs. Leaky ReLU gradient statistics\n",
    "- [ ] Plot: Gradient magnitude for 5-layer network showing potential vanishing/exploding gradients\n",
    "- [ ] Written analysis (3-5 sentences) on gradient flow observations\n",
    "\n",
    "**Specifications:**\n",
    "- Record gradients for at least 100 training steps\n",
    "- Use L2 norm (Frobenius norm for weight matrices) as the gradient magnitude metric\n",
    "- For the 5-layer network, use the same hidden layer size as the 2-layer network\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Correct gradient extraction implementation (0.2 pt)\n",
    "- Clear gradient flow visualizations (0.2 pt)\n",
    "- Depth comparison and vanishing gradient analysis (0.1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 9: Extra (0 pt)\n",
    "\n",
    "No need to turn anything in for this part.\n",
    "\n",
    "- Go through a git tutorial and set up a GitHub account.\n",
    "- Sign-up with Kaggle and explore competitions and datasets that interest you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hints\n",
    "\n",
    "### Question 1 Hints\n",
    "- Break down the function into atomic operations: power, addition, multiplication, division, squaring\n",
    "- Useful derivatives: $\\frac{d}{dx}a^x = a^x \\ln(a)$, $\\frac{d}{da}a^b = b \\cdot a^{b-1}$\n",
    "- At $(1,1,1,1,1)$: $a^b = 1$, $c^d = 1$, so inner terms simplify nicely\n",
    "- Use variable substitution to track intermediate values (e.g., let $u = a^b$, $v = c^d$, etc.)\n",
    "\n",
    "### Question 2 Hints\n",
    "- MSE formula: $\\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2$\n",
    "- For gradient descent, derive $\\frac{\\partial \\text{MSE}}{\\partial m}$ and $\\frac{\\partial \\text{MSE}}{\\partial c}$ analytically\n",
    "- Learning rate too high → oscillation/divergence; too low → slow convergence\n",
    "- Use `np.random.uniform(low, high, size)` to generate data\n",
    "- For tanh model, remember $\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)$\n",
    "\n",
    "### Question 3 Hints\n",
    "- Softmax: $\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K}e^{z_k}}$\n",
    "- Cross-entropy: $L = -\\sum_{k=1}^{K} y_k \\log(\\hat{y}_k)$ where $y$ is one-hot encoded\n",
    "- Numerical stability: Subtract $\\max(z)$ from logits before computing softmax\n",
    "- L1 regularization: $\\lambda_1 \\sum |W_{ij}|$; L2 regularization: $\\lambda_2 \\sum W_{ij}^2$\n",
    "\n",
    "### Question 4 Hints\n",
    "- Reference: [`Data_Linear_Classifier.ipynb`](examples/M02_feedforward/Data_Linear_Classifier.ipynb) and [`Linear_Classifier_Example.ipynb`](examples/M02_feedforward/Linear_Classifier_Example.ipynb)\n",
    "- Use `sklearn.model_selection.train_test_split` with `random_state` for reproducibility\n",
    "- Use `sklearn.model_selection.KFold` or `cross_val_score` for cross-validation\n",
    "- For regularization search, try logarithmic spacing: `np.logspace(-4, 1, 10)`\n",
    "\n",
    "### Question 5 Hints\n",
    "- Keras Leaky ReLU: `tf.keras.layers.LeakyReLU(alpha=0.01)`\n",
    "- Optimizers: `tf.keras.optimizers.SGD()`, `tf.keras.optimizers.Adam()`, `tf.keras.optimizers.RMSprop()`\n",
    "- Reference: [`FFN_Classifier_Example.ipynb`](examples/M02_feedforward/FFN_Classifier_Example.ipynb)\n",
    "- Fashion MNIST: `tf.keras.datasets.fashion_mnist.load_data()`\n",
    "\n",
    "### Question 6 Hints\n",
    "- Create mesh: `xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))`\n",
    "- Predict on mesh: `Z = model.predict(np.c_[xx.ravel(), yy.ravel()])`\n",
    "- Use `plt.contourf(xx, yy, Z.reshape(xx.shape), alpha=0.8, cmap='RdYlBu')`\n",
    "\n",
    "### Question 7 Hints\n",
    "- Set random seed: `tf.random.set_seed(42); np.random.seed(42)`\n",
    "- SGD with momentum: `tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)`\n",
    "- Use Keras `History` object to extract training curves: `history.history['loss']`, `history.history['val_accuracy']`\n",
    "\n",
    "### Question 8 Hints\n",
    "- Use `tf.GradientTape` to compute gradients manually\n",
    "- Access layer weights: `model.layers[i].trainable_weights`\n",
    "- Compute gradient norm: `tf.norm(grad).numpy()`\n",
    "- Keras callback for gradient logging:\n",
    "```python\n",
    "class GradientLogger(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        # Record gradients here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## FAQ\n",
    "\n",
    "**Q: Do I need a GPU for this assignment?**  \n",
    "A: No, all tasks can be completed on CPU. Fashion MNIST training should take ~5-10 minutes per configuration on a modern laptop CPU. If you have access to Google Colab (free GPU), it will be faster but is not required.\n",
    "\n",
    "**Q: How long should training take for Question 5?**  \n",
    "A: Approximately 30 seconds to 2 minutes per model configuration on CPU. If training takes significantly longer, check your batch size (use 64 or 128) and consider reducing epochs for initial debugging.\n",
    "\n",
    "**Q: For Question 1, can I use a computational graph library?**  \n",
    "A: No, draw the graph by hand (or using drawing software like draw.io/diagrams.net). The goal is to understand the manual process. You may verify your answer using PyTorch/TensorFlow autograd.\n",
    "\n",
    "**Q: What constitutes \"appropriate learning rate\" in Question 2?**  \n",
    "A: A learning rate that allows convergence within 1000 iterations. Start with 0.01 and adjust. You should observe decreasing loss. If loss explodes, decrease learning rate; if loss decreases too slowly, increase it.\n",
    "\n",
    "**Q: For cross-validation, should I use stratified folds?**  \n",
    "A: Yes, use `StratifiedKFold` to maintain class balance in each fold. This is especially important if classes are imbalanced.\n",
    "\n",
    "**Q: Can I use sklearn for the linear classifier in Question 4?**  \n",
    "A: You should implement gradient descent yourself (not use `sklearn.linear_model.LogisticRegression`). However, you may use sklearn for data splitting, cross-validation utilities, and accuracy metrics.\n",
    "\n",
    "**Q: What if my decision boundaries look noisy in Question 6?**  \n",
    "A: Increase mesh grid resolution (try 200x200). Also, ensure your classifier is well-trained before visualizing.\n",
    "\n",
    "**Q: How should I handle vanishing gradients in Question 8?**  \n",
    "A: This is an observation exercise. If gradients vanish (become very small in early layers), note this phenomenon. You don't need to fix it, but you should describe what you observe.\n",
    "\n",
    "**Q: For Question 7, what if an optimizer never reaches 85% accuracy?**  \n",
    "A: Report \"Did not reach\" in your table and discuss why (e.g., learning rate too low, optimizer not suitable for this problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "### Lecture Slides\n",
    "- [`M01_review.pdf`](slides/M01_review.pdf) - Backpropagation theory and computation graphs\n",
    "- [`M02_feedforward.pdf`](slides/M02_feedforward.pdf) - Feedforward neural networks, activation functions, optimizers\n",
    "\n",
    "### Example Notebooks\n",
    "- [`Data_Linear_Classifier.ipynb`](examples/M02_feedforward/Data_Linear_Classifier.ipynb) - Data generation for Question 4\n",
    "- [`Linear_Classifier_Example.ipynb`](examples/M02_feedforward/Linear_Classifier_Example.ipynb) - Linear classification reference\n",
    "- [`FFN_Classifier_Example.ipynb`](examples/M02_feedforward/FFN_Classifier_Example.ipynb) - Feedforward network example for Question 5\n",
    "- [`Python_Review_IDS576.ipynb`](examples/M01_basics/Python_Review_IDS576.ipynb) - Python basics refresher\n",
    "- [`Torch_Prelims.ipynb`](examples/M01_basics/Torch_Prelims.ipynb) - PyTorch fundamentals\n",
    "\n",
    "### External Documentation\n",
    "- [NumPy Documentation](https://numpy.org/doc/stable/)\n",
    "- [Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html)\n",
    "- [TensorFlow/Keras Guide](https://www.tensorflow.org/guide/keras)\n",
    "- [Fashion MNIST Dataset](https://www.tensorflow.org/datasets/catalog/fashion_mnist)\n",
    "- [Scikit-learn Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "### Dataset Information\n",
    "- **Question 4 Data**: Generated using `make_classification` from sklearn (see [`Data_Linear_Classifier.ipynb`](examples/M02_feedforward/Data_Linear_Classifier.ipynb))\n",
    "- **Question 5 Data**: Fashion MNIST - automatically downloaded via `tf.keras.datasets.fashion_mnist.load_data()`. Contains 60,000 training and 10,000 test images of 28×28 grayscale clothing items across 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Point Summary\n",
    "\n",
    "| Question | Topic | Points |\n",
    "|----------|-------|--------|\n",
    "| Q1 | Backpropagation | 0.5 |\n",
    "| Q2 | Gradient Descent | 2.0 |\n",
    "| Q3 | ML Basics (Loss Functions) | 0.5 |\n",
    "| Q4 | Classification Pipeline | 2.5 |\n",
    "| Q5 | Feedforward Neural Networks | 2.5 |\n",
    "| Q6 | Decision Boundary Visualization | 0.5 |\n",
    "| Q7 | Optimizer Comparison Study | 1.0 |\n",
    "| Q8 | Gradient Flow Analysis | 0.5 |\n",
    "| Q9 | Extra (Optional) | 0.0 |\n",
    "| **Total** | | **10.0** |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
