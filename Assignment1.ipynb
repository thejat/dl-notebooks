{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDS 576: Assignment 1\n",
    "\n",
    "- Turn in solutions as a single notebook (ipynb) and as a pdf on Blackboard. No need to turn in datasets/word-docs.\n",
    "- Answer the following questions concisely, in complete sentences and with full clarity. If in doubt, ask classmates and the teaching staff. Across group collaboration is not allowed. Always cite all your sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Backpropagation (0.5pt)\n",
    "\n",
    "- Draw the computation graph for the following function: $f(a,b,c,d,e) = \\frac{1}{(1+(a^b + c^d)*e)^2}$. Compute the gradient of the function with respect it to its inputs at $(a,b,c,d,e) = (1,1,1,1,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Descent (2pt)\n",
    "\n",
    " - Write a function to compute the mean squared error between a prediction and ground truth assuming both are numpy arrays (see python module `numpy`).\n",
    " - Consider a model: $y = mx + c$, where the model parameter $m = 1$ and parameter $c = 0$ and $x \\in (0,1)$. Plot the function using matplotlib.\n",
    " - Generate example data by drawing $N = 100$ uniform values from the range in which $x$ lies, and compute the corresponding $y$ to get $\\{x_i,y_i\\}_{i=1}^{N}$.\n",
    " - Assuming that you do not know the model parameters, use backpropagation and gradient descent updates to find the model parameters (choose an appropriate learning rate). The loss function will be the mean squared error.\n",
    " - Plot the error in the estimates as a function of the number of iterations of gradient update. Change the learning rate and plot another curve on the previous plot. \n",
    " - Do steps 3-5 when the model is $y = m_1x + m_2x^2 + c$ and the true parameters are $m_1 = 0.5$, $m_2 = 1$ and $c = 1$. And $x \\in (0,1)$. Also, plot the ground truth function. Compare and contrast the plot with the previous one.\n",
    " - Do steps 3-5 when the model is $y = \\tanh(m*x + c)$ and the true parameters are $m = 1$ and $c = 2$. And $x \\in (0,2)$. Also, plot the ground truth function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ML Basics (0.5pt)\n",
    "\n",
    " - Write a function to compute the ({multiclass}) logistic loss ({also called the cross-entropy loss}) given the parameters $(W,b)$ of a linear model (as \\emph{numpy} arrays) and an example $(x,y)$. \n",
    " - Add an $\\ell_1$ regularization and an $\\ell_2$ regularization to the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classification Pipeline (2.5pt)\n",
    "\n",
    " -  Generate data from \\textit{Data\\_Linear\\_Classifier.ipynb} (refer to the corresponding lecture).\n",
    " -  Split the data into test and train (20\\%:80\\%).\n",
    " -  Build a linear classifier assuming the multiclass logistic loss and an $\\ell_2$ regularization for the weights only. Report the prediction accuracy on the training data and the test data and show appropriate plots.\n",
    " -  Introduce a cross validation scheme and justify your choice of parameters. What is the validation accuracy compare to the test accuracy.\n",
    " -  What is the sensitivity of the model's performance to different learning rates and the number of gradient descent iterations. Describe via suitable plots.\n",
    " -  What is the sensitivity of the model's performance to different regularization parameter values. Find the best regularization parameter using an exhaustive search procedure. Describe your choice via suitable plots. What is the performance difference between using regularization and no regularization?\n",
    " -  What is the sensitivity of the model's performance with respect to a different test train split (e.g., 50\\%:50\\%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feedforward Neural Networks (2.5pt)\n",
    "\n",
    "Consider two models: (a) a 2-layer feedforward neural network (i.e., 1 hidden layer with $f(x,W_1,b_1,W_2,b_2) = W_2\\max(0,W_1x+b_1) + b_2$), and (b) same as before but with leaky ReLU ($f(x) = x$ if  $x > 0$, else $ f(x) = 0.01*x$).\n",
    "\n",
    "\n",
    "\n",
    " -  Build the above classifiers using _Keras_ and _Tensorflow_ and solve the classification problem for [MNIST/Fashion MNIST](https://www.tensorflow.org/tutorials/keras/classification?hl=lv). \n",
    " -  Discuss how optimizer choice influences performance.\n",
    " -  What happens when the number of hidden units chosen is much smaller. Similarly, what happens when the number of hidden units chosen is much higher?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Extra (0pt)\n",
    "No need to turn anything in for this part.\n",
    " -  Go through a git tutorial and set up a github account.\n",
    " -  Sign-up with Kaggle and explore competitions and datasets that interest you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
