{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDS 576: Assignment 2 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By completing this assignment, you will:\n",
    "\n",
    "1. **Understand transfer learning**: Learn how to leverage pretrained models (ResNet18) for feature extraction and finetuning on new classification tasks, understanding when each approach is appropriate.\n",
    "\n",
    "2. **Compare training strategies**: Analyze the trade-offs between using frozen feature extractors versus end-to-end finetuning in terms of accuracy, training time, and computational resources.\n",
    "\n",
    "3. **Implement embedding models**: Build and train embedding representations for recommendation systems using gradient-based optimization, applying concepts from word embeddings to a new domain.\n",
    "\n",
    "4. **Visualize high-dimensional representations**: Apply dimensionality reduction techniques (t-SNE) to interpret and evaluate learned embeddings, connecting visualization to model understanding.\n",
    "\n",
    "5. **Analyze model behavior**: Investigate how hyperparameters (embedding dimension, learning rate) and data characteristics (popularity, cold-start) affect model performance and recommendation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Guidelines\n",
    "\n",
    "- **Format**: Submit both a Jupyter notebook (`.ipynb`) and PDF version on the submission site.\n",
    "- **Naming Convention**: `Assignment2_GroupNumber.ipynb` and `Assignment2_GroupNumber.pdf`\n",
    "- **Structure**: Organize your notebook with clear section headers matching the question numbers (Q1, Q2, etc.). Each question should have:\n",
    "  - Code cells with comments\n",
    "  - Markdown cells explaining your approach and findings\n",
    "  - Output cells showing results/figures (do not clear outputs before submission)\n",
    "- **Citations**: Always cite all sources (papers, tutorials, Stack Overflow, etc.)\n",
    "- **Collaboration**: Within-group discussion allowed; cross-group collaboration is **not** allowed.\n",
    "- **No need to submit**: Datasets, word documents, or external files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 1: CNNs and Finetuning (4 pt)\n",
    "\n",
    "In this question, you will explore transfer learning using a pretrained ResNet18 model on the CIFAR-10 dataset. You will compare two approaches: using the pretrained model as a fixed feature extractor versus finetuning the entire network.\n",
    "\n",
    "**Dataset:**\n",
    "Download the CIFAR-10 dataset (original data can be found [here](http://www.cs.toronto.edu/~kriz/cifar.html), and here is a link to the pickled [python version](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A: Feature Extraction (2 pt)\n",
    "\n",
    "Use the pretrained ResNet18 model (from `torchvision.models`) to extract features. Freeze all layers and use the extracted features as inputs to a new multi-class logistic regression model (use `nn.Linear`/`nn.Module` to define your model).\n",
    "\n",
    "**Sub-questions:**\n",
    "- (a) Describe the preprocessing steps applied to CIFAR-10 images to match ResNet18's expected input format. Report the final test accuracy. (0.5 pt)\n",
    "- (b) Train the logistic regression classifier for at least 10 epochs. Plot the training and validation loss curves. Report train, validation, and test accuracy. (0.5 pt)\n",
    "- (c) Display the top 5 correct predictions and the top 5 incorrect predictions for each of the 10 classes. Show the images with their predicted and true labels in a compact grid format. (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B: Finetuning (2 pt)\n",
    "\n",
    "Finetune the ResNet18 model's parameters (unfreeze some or all layers) and repeat the analysis from Part A.\n",
    "\n",
    "**Sub-questions:**\n",
    "- (a) Describe your finetuning strategy (which layers were unfrozen, learning rate choices, number of epochs). Report the final test accuracy. (0.5 pt)\n",
    "- (b) Plot the training and validation loss curves. Report train, validation, and test accuracy. (0.5 pt)\n",
    "- (c) Display the top 5 correct predictions and the top 5 incorrect predictions for each of the 10 classes (same format as Part A). (0.5 pt)\n",
    "- (d) Create a comparison table showing: training time, number of trainable parameters, and test accuracy for both approaches (feature extraction vs. finetuning). Discuss when you would prefer one approach over the other. (0.5 pt)\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Code cells showing data loading, preprocessing, and model definition\n",
    "- [ ] Training/validation loss plots for both approaches\n",
    "- [ ] Grid visualizations of correct/incorrect predictions (10 classes \u00d7 5 images \u00d7 2 categories = 100 images per approach)\n",
    "- [ ] Comparison table with metrics for both approaches\n",
    "- [ ] Written analysis comparing the two approaches (at least 3-4 sentences)\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Full credit: All deliverables present, code runs without errors, visualizations are clear and properly labeled, analysis demonstrates understanding of transfer learning concepts\n",
    "- Partial credit: Missing deliverables, unclear visualizations, or superficial analysis\n",
    "- No credit: Code does not run or fundamental misunderstanding of the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 2: Movie Embeddings (4 pt)\n",
    "\n",
    "Instead of embedding words, we will embed movies. If we can embed movies, then similar movies will be close to each other and can be recommended. This reasoning is analogous to the [distributional hypothesis of word meanings](https://en.wikipedia.org/wiki/Distributional_semantics). For words, this roughly translates to: words that appear in similar sentences should have similar vector representations. For movies, vectors for two movies should be similar if they are watched by similar people.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Let the total number of movies be $M$. Let $X_{i,j}$ be the number of users that liked both movies $i$ and $j$. We want to obtain vectors $v_1,...,v_i,...,v_j,...,v_M$ for all movies such that we minimize the cost:\n",
    "\n",
    "$$c(v_1,...,v_M) = \\sum_{i=1}^{M}\\sum_{j=1}^{M}\\mathbf{1}_{[i\\neq j]}(v_i^Tv_j - X_{i,j})^2$$\n",
    "\n",
    "Here $\\mathbf{1}_{[i\\neq j]}$ is an indicator function that is $0$ when $i=j$ and $1$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A: Data Preparation (0.5 pt)\n",
    "\n",
    "Compute the co-occurrence matrix $X_{i,j}$ from the MovieLens (small) [dataset](https://files.grouplens.org/datasets/movielens/ml-latest-small.zip). You can also download using the link to `ml-latest-small.zip` from this [page](https://grouplens.org/datasets/movielens/) (be sure to read the corresponding [description](https://files.grouplens.org/datasets/movielens/ml-latest-small-README.html)).\n",
    "\n",
    "**Sub-questions:**\n",
    "- (a) Describe your data preprocessing workflow. How did you define \"liked\" (what rating threshold did you use)? Report the number of movies, users, and non-zero entries in your co-occurrence matrix.\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Code cells showing data loading and preprocessing\n",
    "- [ ] Summary statistics: number of movies, users, ratings, and co-occurrence matrix sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B: Optimization (1.5 pt)\n",
    "\n",
    "Optimize function $c(v_1,...,v_M)$ over $v_1,...,v_M$ using gradient descent (using PyTorch or TensorFlow).\n",
    "\n",
    "**Sub-questions:**\n",
    "- (a) Implement the loss function and training loop. Use an embedding dimension of 50 as your baseline. (0.5 pt)\n",
    "- (b) Plot the loss as a function of iteration for at least 3 different learning rates (e.g., 0.001, 0.01, 0.1). Train for at least 100 epochs. (0.5 pt)\n",
    "- (c) Compare at least 2 different optimizers (e.g., SGD, Adam, RMSprop). Plot the loss curves on the same figure with a legend. Discuss which optimizer converges faster and why. (0.5 pt)\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Code cells with loss function implementation and training loop\n",
    "- [ ] Loss vs. iteration plot comparing learning rates\n",
    "- [ ] Loss vs. iteration plot comparing optimizers\n",
    "- [ ] Written discussion of optimizer comparison (at least 2-3 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C: Movie Recommendations (2 pt)\n",
    "\n",
    "Recommend the top 10 movies (not vectors or indices, but movie names) for the following query movies:\n",
    "- (a) _Apollo 13_\n",
    "- (b) _Toy Story_\n",
    "- (c) _Home Alone_\n",
    "\n",
    "**Sub-questions:**\n",
    "- (a) Describe your recommendation strategy. How do you find similar movies given a query movie's embedding vector? (0.5 pt)\n",
    "- (b) Present the recommendations for each query movie in a clear table format. (0.5 pt)\n",
    "- (c) Do the recommendations change when you change learning rates or optimizers? Run the experiment with at least 2 different configurations and report the results. Explain why the recommendations are stable or unstable. (1 pt)\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Description of similarity/recommendation strategy\n",
    "- [ ] Three tables showing top 10 recommendations for each query movie\n",
    "- [ ] Comparison of recommendations across different training configurations\n",
    "- [ ] Written analysis explaining recommendation stability (at least 3-4 sentences)\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Full credit: All deliverables present, loss curves show convergence, recommendations are sensible and well-presented, analysis demonstrates understanding of embedding optimization\n",
    "- Partial credit: Missing deliverables, unconverged training, or superficial analysis\n",
    "- No credit: Code does not run or recommendations are clearly incorrect (e.g., returning random movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 3: Embedding Visualization (1 pt)\n",
    "\n",
    "Visualize the learned movie embeddings using dimensionality reduction to understand what the model has learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A: t-SNE Visualization (0.5 pt)\n",
    "\n",
    "Apply t-SNE to reduce the movie embedding vectors to 2 dimensions and create a scatter plot.\n",
    "\n",
    "**Sub-questions:**\n",
    "- (a) Apply t-SNE with perplexity=30 to your learned movie embeddings. Create a scatter plot of the 2D projections.\n",
    "- (b) Color-code the points by movie genre (use the primary genre from the MovieLens dataset). Include a legend.\n",
    "- (c) Annotate at least 10 well-known movies on the plot (e.g., the query movies from Question 2 plus others).\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] t-SNE scatter plot with genre color-coding\n",
    "- [ ] Legend showing genre-to-color mapping\n",
    "- [ ] Annotations for at least 10 recognizable movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B: Cluster Analysis (0.5 pt)\n",
    "\n",
    "Analyze the structure of the embedding space.\n",
    "\n",
    "**Sub-questions:**\n",
    "- (a) Identify 2-3 clusters of movies that appear close together in the t-SNE visualization. List the movies in each cluster and explain what they have in common (genre, era, themes, etc.).\n",
    "- (b) Find 2 movies that are close in embedding space but seem different on the surface. Hypothesize why the model considers them similar.\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Description of 2-3 identified clusters with movie lists\n",
    "- [ ] Analysis of 2 surprising similar movies with hypothesis\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Full credit: Clear, well-labeled visualization with insightful cluster analysis\n",
    "- Partial credit: Visualization present but analysis is superficial\n",
    "- No credit: No visualization or completely incorrect implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 4: Embedding Analysis (1 pt)\n",
    "\n",
    "Investigate how design choices affect the quality of learned embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A: Embedding Dimension Ablation (0.5 pt)\n",
    "\n",
    "Study how the embedding dimension affects model performance and recommendations.\n",
    "\n",
    "**Sub-questions:**\n",
    "- (a) Train embedding models with dimensions: 10, 25, 50, 100, and 200. Use the same optimizer and learning rate for fair comparison.\n",
    "- (b) Create a table showing: embedding dimension, final training loss, and training time for each configuration.\n",
    "- (c) For one query movie (e.g., Toy Story), show how the top 5 recommendations change across different embedding dimensions.\n",
    "- (d) Discuss the trade-offs: When might you prefer smaller vs. larger embedding dimensions?\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Table comparing embedding dimensions (loss, time)\n",
    "- [ ] Comparison of recommendations across dimensions for one query movie\n",
    "- [ ] Written analysis of dimension trade-offs (at least 2-3 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B: Cold-Start Problem Analysis (0.5 pt)\n",
    "\n",
    "Analyze how the model handles movies with few ratings.\n",
    "\n",
    "**Sub-questions:**\n",
    "- (a) Identify 5 movies with very few ratings (< 10 ratings) and 5 movies with many ratings (> 100 ratings) in the dataset.\n",
    "- (b) For each of these 10 movies, find their 3 nearest neighbors in embedding space and report them.\n",
    "- (c) Compare the quality of recommendations for popular vs. unpopular movies. Are the recommendations equally sensible? Why or why not?\n",
    "\n",
    "**Deliverables:**\n",
    "- [ ] Table of 5 low-rating and 5 high-rating movies with their rating counts\n",
    "- [ ] Nearest neighbor recommendations for each of the 10 movies\n",
    "- [ ] Written analysis comparing recommendation quality (at least 3-4 sentences)\n",
    "\n",
    "**Grading Criteria:**\n",
    "- Full credit: Thorough ablation study with insightful analysis of results\n",
    "- Partial credit: Experiments run but analysis is superficial\n",
    "- No credit: Missing experiments or no analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 Hints\n",
    "\n",
    "**Data Loading:**\n",
    "- Use `torchvision.datasets.CIFAR10` for easy data loading\n",
    "- CIFAR-10 images are 32\u00d732, but ResNet18 expects 224\u00d7224 input. Use `transforms.Resize(224)` or `transforms.Resize(256)` followed by `transforms.CenterCrop(224)`\n",
    "- Apply ImageNet normalization: `transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`\n",
    "\n",
    "**Feature Extraction:**\n",
    "- Load pretrained model: `models.resnet18(pretrained=True)` or `models.resnet18(weights='IMAGENET1K_V1')`\n",
    "- To extract features, remove the final fully connected layer or use a forward hook\n",
    "- Freeze parameters: `for param in model.parameters(): param.requires_grad = False`\n",
    "- ResNet18 produces 512-dimensional feature vectors (before the final FC layer)\n",
    "\n",
    "**Finetuning:**\n",
    "- Use a smaller learning rate for pretrained layers (e.g., 1e-4) and larger for the new classifier head (e.g., 1e-3)\n",
    "- Consider unfreezing only the last few layers initially\n",
    "- Use `torch.optim.lr_scheduler` for learning rate scheduling\n",
    "\n",
    "**Visualization:**\n",
    "- Use `matplotlib.pyplot.subplots()` with appropriate grid dimensions\n",
    "- Remember to denormalize images before displaying: `img = img * std + mean`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 Hints\n",
    "\n",
    "**Data Preparation:**\n",
    "- Read `ratings.csv` using `pandas.read_csv()`\n",
    "- A common threshold for \"liked\" is rating \u2265 4.0 (out of 5)\n",
    "- Build a user-movie matrix first, then compute co-occurrence: $X = A^T A$ where $A$ is the binary user-movie \"liked\" matrix\n",
    "- The diagonal of $X$ counts how many users liked each movie; ignore it in the loss\n",
    "\n",
    "**Optimization:**\n",
    "- Define embeddings using `nn.Embedding(num_movies, embedding_dim)` or `nn.Parameter(torch.randn(num_movies, embedding_dim))`\n",
    "- Compute the loss efficiently using matrix operations: `predictions = embeddings @ embeddings.T`\n",
    "- Create a mask to exclude diagonal elements: `mask = ~torch.eye(M, dtype=bool)`\n",
    "- Start with a smaller subset of movies if training is slow\n",
    "\n",
    "**Recommendation:**\n",
    "- Use cosine similarity or Euclidean distance to find similar movies\n",
    "- `torch.nn.functional.cosine_similarity()` or `sklearn.metrics.pairwise.cosine_similarity()`\n",
    "- Use `movies.csv` to map movie IDs to titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 Hints\n",
    "\n",
    "**t-SNE:**\n",
    "- Use `sklearn.manifold.TSNE(n_components=2, perplexity=30, random_state=42)`\n",
    "- For large datasets, consider using `n_iter=1000` for better convergence\n",
    "- t-SNE is stochastic; set `random_state` for reproducibility\n",
    "\n",
    "**Genre Coloring:**\n",
    "- Read `movies.csv` for genre information\n",
    "- Movies can have multiple genres; use the first listed genre as \"primary\"\n",
    "- Use a categorical colormap: `plt.cm.tab20` or `sns.color_palette(\"husl\", n_genres)`\n",
    "\n",
    "**Annotations:**\n",
    "- Use `plt.annotate()` or `ax.text()` to label points\n",
    "- Offset labels slightly to avoid overlapping with points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 Hints\n",
    "\n",
    "**Ablation Study:**\n",
    "- Keep all hyperparameters except embedding dimension constant\n",
    "- Train for the same number of epochs for fair comparison\n",
    "- Use `time.time()` to measure training time\n",
    "\n",
    "**Cold-Start Analysis:**\n",
    "- Count ratings per movie: `ratings_df.groupby('movieId').size()`\n",
    "- Popular movies should have better embeddings due to more training signal\n",
    "- Consider: What happens when a movie only co-occurs with a few others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## FAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Do I need a GPU for this assignment?**  \n",
    "A: A GPU will significantly speed up training, especially for Question 1 (CNN finetuning). However, the assignment is completable on CPU. Expected times:\n",
    "- Question 1 (feature extraction): ~10 min on CPU, ~1 min on GPU\n",
    "- Question 1 (finetuning): ~30-60 min on CPU, ~5 min on GPU\n",
    "- Question 2-4 (embeddings): ~5-10 min on CPU, faster on GPU\n",
    "\n",
    "**Q: What rating threshold should I use for \"liked\" in Question 2?**  \n",
    "A: You have flexibility here. Common choices are \u2265 4.0 or \u2265 3.5. Document your choice and justify it briefly. The key is to be consistent.\n",
    "\n",
    "**Q: How many epochs should I train for?**  \n",
    "A: For Question 1, train until the validation loss plateaus (typically 10-25 epochs). For Question 2, train until the loss converges (typically 100-500 epochs depending on learning rate).\n",
    "\n",
    "**Q: What if a query movie isn't in my dataset?**  \n",
    "A: The three query movies (Apollo 13, Toy Story, Home Alone) are all present in the MovieLens small dataset. If you have issues finding them, check for exact title matching (case-sensitive) and year information in the title.\n",
    "\n",
    "**Q: How should I handle movies with multiple genres?**  \n",
    "A: For visualization purposes, use the first genre listed as the primary genre. You may also try other approaches (e.g., one-hot encoding multiple genres) and discuss the differences.\n",
    "\n",
    "**Q: What if my t-SNE visualization looks random/unstructured?**  \n",
    "A: This could indicate: (1) embeddings haven't converged \u2013 train longer, (2) perplexity is too high/low \u2013 try perplexity in [5, 50], (3) the embedding model isn't learning meaningful representations \u2013 check your loss is decreasing.\n",
    "\n",
    "**Q: Should I normalize embeddings before computing similarity?**  \n",
    "A: Yes, for cosine similarity the vectors are normalized. For Euclidean distance, normalization is optional but can help. Be consistent and document your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture Slides\n",
    "- [`M03a_cnn_and_transfer.pdf`](slides/M03a_cnn_and_transfer.pdf) \u2013 CNNs, transfer learning, feature extraction, and finetuning strategies\n",
    "- [`M04_text.pdf`](slides/M04_text.pdf) \u2013 Word embeddings and distributional semantics (concepts applicable to movie embeddings)\n",
    "\n",
    "### Example Notebooks\n",
    "- [`ConvolutionalNet_Classifier_Example.ipynb`](examples/M03_cnn_transfer/ConvolutionalNet_Classifier_Example.ipynb) \u2013 Transfer learning with ResNet, feature extraction vs. finetuning\n",
    "- [`TSNE_Embedding_Example_MNIST.ipynb`](examples/M03_cnn_transfer/TSNE_Embedding_Example_MNIST.ipynb) \u2013 t-SNE visualization of embeddings\n",
    "- [`FFN_Classifier_Example.ipynb`](examples/M02_feedforward/FFN_Classifier_Example.ipynb) \u2013 PyTorch training loop patterns\n",
    "\n",
    "### External Documentation\n",
    "\n",
    "**PyTorch:**\n",
    "- [torchvision.models.resnet18](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html)\n",
    "- [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "- [Transfer Learning Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "\n",
    "**Scikit-learn:**\n",
    "- [sklearn.manifold.TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n",
    "- [Manifold Learning Examples](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html)\n",
    "\n",
    "### Dataset Links\n",
    "- [CIFAR-10 Dataset](http://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "- [CIFAR-10 Python Version (direct download)](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)\n",
    "- [MovieLens Small Dataset](https://files.grouplens.org/datasets/movielens/ml-latest-small.zip)\n",
    "- [MovieLens Dataset Description](https://files.grouplens.org/datasets/movielens/ml-latest-small-README.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Point Summary\n",
    "\n",
    "| Question | Topic | Points |\n",
    "|----------|-------|--------|\n",
    "| Q1 | CNNs and Finetuning | 4.0 |\n",
    "| Q2 | Movie Embeddings | 4.0 |\n",
    "| Q3 | Embedding Visualization | 1.0 |\n",
    "| Q4 | Embedding Analysis | 1.0 |\n",
    "| **Total** | | **10.0** |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}